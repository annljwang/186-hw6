{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activations.py\n",
    "\n",
    "\"\"\"\n",
    "Author: Sophia Sanborn\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Activation(ABC):\n",
    "    \"\"\"Abstract class defining the common interface for all activation methods.\"\"\"\n",
    "\n",
    "    def __call__(self, Z):\n",
    "        return self.forward(Z)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, Z):\n",
    "        pass\n",
    "\n",
    "\n",
    "def initialize_activation(name: str) -> Activation:\n",
    "    \"\"\"Factory method to return an Activation object of the specified type.\"\"\"\n",
    "    if name == \"linear\":\n",
    "        return Linear()\n",
    "    elif name == \"sigmoid\":\n",
    "        return Sigmoid()\n",
    "    elif name == \"tanh\":\n",
    "        return TanH()\n",
    "    elif name == \"arctan\":\n",
    "        return ArcTan()\n",
    "    elif name == \"relu\":\n",
    "        return ReLU()\n",
    "    elif name == \"softmax\":\n",
    "        return SoftMax()\n",
    "    else:\n",
    "        raise NotImplementedError(\"{} activation is not implemented\".format(name))\n",
    "\n",
    "\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        return dY\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for sigmoid function:\n",
    "        f(z) = 1 / (1 + exp(-z))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for sigmoid.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "\n",
    "class TanH(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = tanh(z).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return 2 / (1 + np.exp(-2 * Z)) - 1\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = tanh(z).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        fn = self.forward(Z)\n",
    "        return dY * (1 - fn ** 2)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        out = np.maximum(0, Z)\n",
    "        return out\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return dY * np.where(Z > 0, 1, 0)\n",
    "\n",
    "\n",
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for softmax activation.\n",
    "        Hint: The naive implementation might not be numerically stable.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = np.max(Z, axis=1).reshape(-1, 1)\n",
    "        sm = Z - m\n",
    "        \n",
    "        exp_sm = np.exp(sm)\n",
    "        \n",
    "        return exp_sm / np.sum(exp_sm, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for softmax activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        dLdZ = np.zeros(Z.shape)\n",
    "        softmaxZ = self.forward(Z)\n",
    "        \n",
    "        for i in range(0, softmaxZ.shape[0]):\n",
    "            curr_sample = softmaxZ[i, :][:, None]\n",
    "            curr_dY = dY[i, :][:, None]\n",
    "            j = np.matmul(-curr_sample, curr_sample.T)\n",
    "            \n",
    "            np.fill_diagonal(j, np.array([s*(1-s) for s in curr_sample]))\n",
    "            dLdZ[i, :][:, None] = np.matmul(j, curr_dY)\n",
    "        \n",
    "        return dLdZ\n",
    "\n",
    "\n",
    "class ArcTan(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z):\n",
    "        return np.arctan(Z)\n",
    "\n",
    "    def backward(self, Z, dY):\n",
    "        return dY * 1 / (Z ** 2 + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn, Sagnik Bhattacharya\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas, github.com/sagnibak\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from neural_networks.activations import initialize_activation\n",
    "from neural_networks.weights import initialize_weights\n",
    "from collections import OrderedDict\n",
    "from neural_networks.utils.convolution import pad2d\n",
    "\n",
    "from typing import Callable, List, Literal, Tuple, Union\n",
    "\n",
    "\n",
    "class Layer(ABC):\n",
    "    \"\"\"Abstract class defining the `Layer` interface.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = None\n",
    "\n",
    "        self.n_in = None\n",
    "        self.n_out = None\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def clear_gradients(self) -> None:\n",
    "        self.cache = OrderedDict({a: [] for a, b in self.cache.items()})\n",
    "        self.gradients = OrderedDict(\n",
    "            {a: np.zeros_like(b) for a, b in self.gradients.items()}\n",
    "        )\n",
    "\n",
    "    def forward_with_param(\n",
    "        self, param_name: str, X: np.ndarray,\n",
    "    ) -> Callable[[np.ndarray], np.ndarray]:\n",
    "        \"\"\"Call the `forward` method but with `param_name` as the variable with\n",
    "        value `param_val`, and keep `X` fixed.\n",
    "        \"\"\"\n",
    "\n",
    "        def inner_forward(param_val: np.ndarray) -> np.ndarray:\n",
    "            self.parameters[param_name] = param_val\n",
    "            return self.forward(X)\n",
    "\n",
    "        return inner_forward\n",
    "\n",
    "    def _get_parameters(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.parameters.items()]\n",
    "\n",
    "    def _get_cache(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.cache.items()]\n",
    "\n",
    "    def _get_gradients(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.gradients.items()]\n",
    "\n",
    "\n",
    "def initialize_layer(\n",
    "    name: str,\n",
    "    activation: str = None,\n",
    "    weight_init: str = None,\n",
    "    n_out: int = None,\n",
    "    kernel_shape: Tuple[int, int] = None,\n",
    "    stride: int = None,\n",
    "    pad: int = None,\n",
    "    mode: str = None,\n",
    "    keep_dim: str = \"first\",\n",
    ") -> Layer:\n",
    "    \"\"\"Factory function for layers.\"\"\"\n",
    "    if name == \"fully_connected\":\n",
    "        return FullyConnected(\n",
    "            n_out=n_out, activation=activation, weight_init=weight_init,\n",
    "        )\n",
    "\n",
    "    elif name == \"conv2d\":\n",
    "        return Conv2D(\n",
    "            n_out=n_out,\n",
    "            activation=activation,\n",
    "            kernel_shape=kernel_shape,\n",
    "            stride=stride,\n",
    "            pad=pad,\n",
    "            weight_init=weight_init,\n",
    "        )\n",
    "\n",
    "    elif name == \"pool2d\":\n",
    "        return Pool2D(kernel_shape=kernel_shape, mode=mode, stride=stride, pad=pad)\n",
    "\n",
    "    elif name == \"flatten\":\n",
    "        return Flatten(keep_dim=keep_dim)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Layer type {} is not implemented\".format(name))\n",
    "\n",
    "\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.init_weights((self.n_in, self.n_out))\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache: OrderedDict = {}  # cache for backprop\n",
    "        self.gradients: OrderedDict = OrderedDict({\"W\": np.zeros(W.shape), \"b\": np.zeros(b.shape)})\n",
    "            # parameter gradients initialized to zero\n",
    "            # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    " \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # perform an affine transformation and activation\n",
    "        Z = np.matmul(X, self.parameters[\"W\"]) + self.parameters[\"b\"]\n",
    "        out = self.activation(Z)\n",
    "        \n",
    "        # store information necessary for backprop in `self.cache`\n",
    "        self.cache = {\"X\": X, \"Z\": Z}\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "       \n",
    "        # unpack the cache\n",
    "        X= self.cache[\"X\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "        \n",
    "        dLdZ = self.activation.backward(Z, dLdY)\n",
    "        dLdW = np.matmul(X.T, dLdZ)\n",
    "        dLdb = np.matmul(dLdZ.T, np.ones((batch_size, 1))).reshape(self.n_out,)\n",
    "        dX = np.matmul(dLdZ, self.parameters[\"W\"].T)\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "        self.gradients = {\n",
    "            \"W\": dLdW,\n",
    "            \"b\": dLdb\n",
    "        }\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    \"\"\"Convolutional layer for inputs with 2 spatial dimensions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        pad: str = \"same\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters and determine padding.\"\"\"\n",
    "        self.n_in = X_shape[3]\n",
    "\n",
    "        W_shape = self.kernel_shape + (self.n_in,) + (self.n_out,)\n",
    "        W = self.init_weights(W_shape)\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache = OrderedDict({\"Z\": [], \"X\": []})\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)})\n",
    "\n",
    "        if self.pad == \"same\":\n",
    "            self.pad = ((W_shape[0] - 1) // 2, (W_shape[1] - 1) // 2)\n",
    "        elif self.pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(self.pad, int):\n",
    "            self.pad = (self.pad, self.pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        padded_x, p = pad2d(X, self.pad, kernel_shape, stride=self.stride)\n",
    "        _, padH, padW, _ = p\n",
    "\n",
    "        # implement a convolutional forward pass\n",
    "        Hout = int(1 + (in_rows + 2*padH - kernel_height) / self.stride)\n",
    "        Wout = int(1 + (in_cols + 2*padW - kernel_width) / self.stride)\n",
    "        Z = np.empty((n_examples,Hout,Wout,out_channels))\n",
    "        \n",
    "        for h in range(Hout):\n",
    "            for wi in range(Wout):\n",
    "                toConvolute = padded_x[:, h*self.stride : h*self.stride+kernel_height, wi*self.stride : wi*self.stride+kernel_width, :]\n",
    "                for f in range(out_channels):\n",
    "                    Z[:, h, wi, f] = np.sum(toConvolute*W[:, :, :, f], axis=(1,2,3)) + b[0, f]\n",
    "                    \n",
    "        out = self.activation(Z)\n",
    "\n",
    "        # cache any values required for backprop\n",
    "        self.cache[\"X\"] = X\n",
    "        self.cache[\"Z\"] = Z\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        \n",
    "        X = self.cache[\"X\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "        \n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        \n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "                \n",
    "                \n",
    "        Hout = dLdY.shape[1]\n",
    "        Wout = dLdY.shape[2]\n",
    "        \n",
    "        padded_x, p = pad2d(X, self.pad, kernel_shape, stride=self.stride)\n",
    "        _, padH, padW, _ = p\n",
    "\n",
    "        padded_dx = np.zeros(padded_x.shape)\n",
    "        dw = np.zeros(W.shape)    \n",
    "        \n",
    "        # perform a backward pass\n",
    "        \n",
    "        dLdY = self.activation.backward(Z, dLdY)\n",
    "        \n",
    "        for i in range(Hout):\n",
    "            for j in range(Wout):\n",
    "                h_start = i* self.stride\n",
    "                h_end = h_start +kernel_height\n",
    "                w_start = j *self.stride\n",
    "                w_end = w_start + kernel_width\n",
    "                padded_dx [:, h_start:h_end, w_start:w_end, :] +=\\\n",
    "                    (W[np.newaxis, :, :, :, :]*dLdY[:, i:i+1, j:j+1, np.newaxis, :]).sum(axis=4)\n",
    "                dw += np.sum(padded_x[:, h_start:h_end, w_start:w_end, :, np.newaxis] *\\\n",
    "                             dLdY[:, i:i+1, j:j+1, np.newaxis, :], axis=0)\n",
    "                    \n",
    "        dx = padded_dx[:,padH:-padH, padW:-padW, :]\n",
    "        db = dLdY.sum(axis=(0, 1, 2)).reshape(1, -1)\n",
    "        \n",
    "        #storing gradients\n",
    "        self.gradients[\"W\"] = dw\n",
    "        self.gradients[\"b\"] = db\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dx\n",
    "    \n",
    "        \n",
    "\n",
    "class Pool2D(Layer):\n",
    "    \"\"\"Pooling layer, implements max and average pooling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        mode: str = \"max\",\n",
    "        stride: int = 1,\n",
    "        pad: Union[int, Literal[\"same\"], Literal[\"valid\"]] = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        if type(kernel_shape) == int:\n",
    "            kernel_shape = (kernel_shape, kernel_shape)\n",
    "\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "\n",
    "        if pad == \"same\":\n",
    "            self.pad = ((kernel_shape[0] - 1) // 2, (kernel_shape[1] - 1) // 2)\n",
    "        elif pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(pad, int):\n",
    "            self.pad = (pad, pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.pool_fn = np.max\n",
    "            self.arg_pool_fn = np.argmax\n",
    "        elif mode == \"average\":\n",
    "            self.pool_fn = np.mean\n",
    "\n",
    "        self.cache = {\n",
    "            \"out_rows\": [],\n",
    "            \"out_cols\": [],\n",
    "            \"X_pad\": [],\n",
    "            \"p\": [],\n",
    "            \"pool_shape\": [],\n",
    "        }\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: use the pooling function to aggregate local information\n",
    "        in the input. This layer typically reduces the spatial dimensionality of\n",
    "        the input while keeping the number of feature maps the same.\n",
    "\n",
    "        As with all other layers, please make sure to cache the appropriate\n",
    "        information for the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input array of shape (batch_size, in_rows, in_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pooled array of shape (batch_size, out_rows, out_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement the forward pass\n",
    "\n",
    "        # cache any values required for backprop\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "    \n",
    "        n, h_in, w_in, c = X.shape\n",
    "        pad_h, pad_w = self.pad\n",
    "        h_pool, w_pool = self.kernel_shape\n",
    "        h_pads = h_pool- 2*pad_h\n",
    "        w_pads= w_pool - 2*pad_w\n",
    "        h_out = 1 + (h_in - h_pads) // self.stride\n",
    "        w_out = 1 + (w_in - w_pads) // self.stride\n",
    "        output = np.zeros((n, h_out, w_out, c))\n",
    "\n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                h_start = i * self.stride\n",
    "                h_end = h_start + h_pads\n",
    "                w_start = j * self.stride\n",
    "                w_end = w_start + w_pads\n",
    "                a_prev_slice = X[:, h_start:h_end, w_start:w_end, :]\n",
    "                #self._save_mask(x=a_prev_slice, cords=(i, j))\n",
    "                output[:, i, j, :] = self.pool_fn(a_prev_slice, axis=(1, 2))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for pooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to the output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a backward pass\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"Flatten the input array.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_dim: str = \"first\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.keep_dim = keep_dim\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.X = []\n",
    "        self.gradients = {}\n",
    "        self.parameters = {}\n",
    "        self.cache = {\"in_dims\": []}\n",
    "\n",
    "    def forward(self, X: np.ndarray, retain_derived: bool = True) -> np.ndarray:\n",
    "        self.cache[\"in_dims\"] = X.shape\n",
    "\n",
    "        if self.keep_dim == -1:\n",
    "            return X.flatten().reshape(1, -1)\n",
    "\n",
    "        rs = (X.shape[0], -1) if self.keep_dim == \"first\" else (-1, X.shape[-1])\n",
    "        return X.reshape(*rs)\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        in_dims = self.cache[\"in_dims\"]\n",
    "        dX = dLdY.reshape(in_dims)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def initialize_loss(name: str) -> Loss:\n",
    "    if name == \"cross_entropy\":\n",
    "        return CrossEntropy(name)\n",
    "    elif name == \"l2\":\n",
    "        return L2(name)\n",
    "    else:\n",
    "        raise NotImplementedError(\"{} loss is not implemented\".format(name))\n",
    "\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Computes the loss for predictions `Y_hat` given one-hot encoded labels\n",
    "        `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = Y.shape[0]\n",
    "        loss = np.sum( np.diag( np.matmul( Y, np.log(Y_hat + 1e-10).T) ) )\n",
    "        \n",
    "        \n",
    "        return (-1/m)*loss\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass of cross-entropy loss.\n",
    "        NOTE: This is correct ONLY when the loss function is SoftMax.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the derivative of the cross-entropy loss with respect to the vector of\n",
    "        predictions, `Y_hat`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = Y.shape[0]\n",
    "        return (-1/m)*Y/(Y_hat+1e-11)\n",
    "\n",
    "\n",
    "class L2(Loss):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mean squared error loss for predictions `Y_hat` given\n",
    "        regression targets `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      vector of regression targets of shape (batch_size, 1)\n",
    "        Y_hat  vector of predictions of shape (batch_size, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for mean squared error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      vector of regression targets of shape (batch_size, 1)\n",
    "        Y_hat  vector of predictions of shape (batch_size, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the derivative of the mean squared error with respect to the last layer\n",
    "        of the neural network\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "from neural_networks.losses import initialize_loss\n",
    "from neural_networks.optimizers import initialize_optimizer\n",
    "from neural_networks.layers import initialize_layer\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# imports for typing only\n",
    "from neural_networks.utils.data_structures import AttrDict\n",
    "from neural_networks.datasets import Dataset\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "def initialize_model(name, loss, layer_args, optimizer_args, logger=None, seed=None):\n",
    "\n",
    "    return NeuralNetwork(\n",
    "        loss=loss,\n",
    "        layer_args=layer_args,\n",
    "        optimizer_args=optimizer_args,\n",
    "        logger=logger,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "class NeuralNetwork(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss: str,\n",
    "        layer_args: Sequence[AttrDict],\n",
    "        optimizer_args: AttrDict,\n",
    "        logger=None,\n",
    "        seed: int = None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.n_layers = len(layer_args)\n",
    "        self.layer_args = layer_args\n",
    "        self.logger = logger\n",
    "        self.epoch_log = {\"loss\": {}, \"error\": {}}\n",
    "\n",
    "        self.loss = initialize_loss(loss)\n",
    "        self.optimizer = initialize_optimizer(**optimizer_args)\n",
    "        self._initialize_layers(layer_args)\n",
    "\n",
    "    def _initialize_layers(self, layer_args: Sequence[AttrDict]) -> None:\n",
    "        self.layers = []\n",
    "        for l_arg in layer_args[:-1]:\n",
    "            l = initialize_layer(**l_arg)\n",
    "            self.layers.append(l)\n",
    "\n",
    "    def _log(self, loss: float, error: float, validation: bool = False) -> None:\n",
    "\n",
    "        if self.logger is not None:\n",
    "            if validation:\n",
    "\n",
    "                self.epoch_log[\"loss\"][\"validate\"] = round(loss, 4)\n",
    "                self.epoch_log[\"error\"][\"validate\"] = round(error, 4)\n",
    "                self.logger.push(self.epoch_log)\n",
    "                self.epoch_log = {\"loss\": {}, \"error\": {}}\n",
    "            else:\n",
    "                self.epoch_log[\"loss\"][\"train\"] = round(loss, 4)\n",
    "                self.epoch_log[\"error\"][\"train\"] = round(error, 4)\n",
    "\n",
    "    def save_parameters(self, epoch: int) -> None:\n",
    "        parameters = {}\n",
    "        for i, l in enumerate(self.layers):\n",
    "            parameters[i] = l.parameters\n",
    "        if self.logger is None:\n",
    "            raise ValueError(\"Must have a logger\")\n",
    "        else:\n",
    "            with open(\n",
    "                self.logger.save_dir + \"parameters_epoch{}\".format(epoch), \"wb\"\n",
    "            ) as f:\n",
    "                pickle.dump(parameters, f)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"One forward pass through all the layers of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  design matrix whose must match the input shape required by the\n",
    "           first layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forward pass output, matches the shape of the output of the last layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Iterate through the network's layers.\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"One backward pass through all the layers of the neural network.\n",
    "        During this phase we calculate the gradients of the loss with respect to\n",
    "        each of the parameters of the entire neural network. Most of the heavy\n",
    "        lifting is done by the `backward` methods of the layers, so this method\n",
    "        should be relatively simple. Also make sure to compute the loss in this\n",
    "        method and NOT in `self.forward`.\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the loss of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the loss.\n",
    "        # Backpropagate through the network's layers.\n",
    "        loss = self.loss(target, out)\n",
    "        \n",
    "        dLdout = self.loss.backward(target, out)\n",
    "        for layer in self.layers[::-1]:\n",
    "            dLdout = layer.backward(dLdout)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def update(self, epoch: int) -> None:\n",
    "        \"\"\"One step of gradient update using the derivatives calculated by\n",
    "        `self.backward`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch  the epoch we are currently on\n",
    "        \"\"\"\n",
    "        param_log = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for param_name, param in layer.parameters.items():\n",
    "                if param_name != \"null\":  # FIXME: possible change needed to `is not`\n",
    "                    param_grad = layer.gradients[param_name]\n",
    "                    # Optimizer needs to keep track of layers\n",
    "                    delta = self.optimizer.update(\n",
    "                        param_name + str(i), param, param_grad, epoch\n",
    "                    )\n",
    "                    layer.parameters[param_name] -= delta\n",
    "                    if self.logger is not None:\n",
    "                        param_log[\"{}{}\".format(param_name, i)] = {}\n",
    "                        param_log[\"{}{}\".format(param_name, i)][\"max\"] = np.max(param)\n",
    "                        param_log[\"{}{}\".format(param_name, i)][\"min\"] = np.min(param)\n",
    "            layer.clear_gradients()\n",
    "        self.epoch_log[\"params\"] = param_log\n",
    "\n",
    "    def error(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"Only calculate the error of the model's predictions given `target`.\n",
    "\n",
    "        For classification tasks,\n",
    "            error = 1 - accuracy\n",
    "\n",
    "        For regression tasks,\n",
    "            error = mean squared error\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on features corresponding to\n",
    "                `target`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the error of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        # classification error\n",
    "        if self.loss.name == \"cross_entropy\":\n",
    "            predictions = np.argmax(out, axis=1)\n",
    "            target_idxs = np.argmax(target, axis=1)\n",
    "            error = np.mean(predictions != target_idxs)\n",
    "\n",
    "        # regression error\n",
    "        elif self.loss.name == \"l2\":\n",
    "            error = np.mean((target - out) ** 2)\n",
    "\n",
    "        # Error!\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Error for {} loss is not implemented\".format(self.loss)\n",
    "            )\n",
    "\n",
    "        return error\n",
    "\n",
    "    def train(self, dataset: Dataset, epochs: int) -> None:\n",
    "        \"\"\"Train the neural network on using the provided dataset for `epochs`\n",
    "        epochs. One epoch comprises one full pass through the entire dataset, or\n",
    "        in case of stochastic gradient descent, one epoch comprises seeing as\n",
    "        many samples from the dataset as there are elements in the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset  training dataset\n",
    "        epochs   number of epochs to train for\n",
    "        \"\"\"\n",
    "        # Initialize output layer\n",
    "        args = self.layer_args[-1]\n",
    "        args[\"n_out\"] = dataset.out_dim\n",
    "        output_layer = initialize_layer(**args)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            training_loss = []\n",
    "            training_error = []\n",
    "            for _ in tqdm(range(dataset.train.samples_per_epoch)):\n",
    "                X, Y = dataset.train.sample()\n",
    "                Y_hat = self.forward(X)\n",
    "                L = self.backward(np.array(Y), np.array(Y_hat))\n",
    "                error = self.error(Y, Y_hat)\n",
    "                self.update(i)\n",
    "                training_loss.append(L)\n",
    "                training_error.append(error)\n",
    "            training_loss = np.mean(training_loss)\n",
    "            training_error = np.mean(training_error)\n",
    "            self._log(training_loss, training_error)\n",
    "\n",
    "            validation_loss = []\n",
    "            validation_error = []\n",
    "            for _ in range(dataset.validate.samples_per_epoch):\n",
    "                X, Y = dataset.validate.sample()\n",
    "                Y_hat = self.forward(X)\n",
    "                L = self.loss.forward(Y, Y_hat)\n",
    "                error = self.error(Y, Y_hat)\n",
    "                validation_loss.append(L)\n",
    "                validation_error.append(error)\n",
    "            validation_loss = np.mean(validation_loss)\n",
    "            validation_error = np.mean(validation_error)\n",
    "            self._log(validation_loss, validation_error, validation=True)\n",
    "\n",
    "            print(\"Example target: {}\".format(Y[0]))\n",
    "            print(\"Example prediction: {}\".format([round(x, 4) for x in Y_hat[0]]))\n",
    "            print(\n",
    "                \"Epoch {} Training Loss: {} Training Accuracy: {} Val Loss: {} Val Accuracy: {}\".format(\n",
    "                    i,\n",
    "                    round(training_loss, 4),\n",
    "                    round(1 - training_error, 4),\n",
    "                    round(validation_loss, 4),\n",
    "                    round(1 - validation_error, 4),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test(\n",
    "        self, dataset: Dataset, save_predictions: bool = False\n",
    "    ) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"Makes predictions on the data in `datasets`, returning the loss, and\n",
    "        optionally returning the predictions and saving both.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset  test data\n",
    "        save_predictions  whether to calculate and save the predictions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a dictionary containing the loss for each data point and optionally also\n",
    "        the prediction for each data point\n",
    "        \"\"\"\n",
    "        test_log = {\"loss\": [], \"error\": []}\n",
    "        if save_predictions:\n",
    "            test_log[\"prediction\"] = []\n",
    "        for _ in range(dataset.test.samples_per_epoch):\n",
    "            X, Y = dataset.test.sample()\n",
    "            Y_hat, L = self.predict(X, Y)\n",
    "            error = self.error(Y, Y_hat)\n",
    "            test_log[\"loss\"].append(L)\n",
    "            test_log[\"error\"].append(error)\n",
    "            if save_predictions:\n",
    "                test_log[\"prediction\"] += [x for x in Y_hat]\n",
    "        test_loss = np.mean(test_log[\"loss\"])\n",
    "        test_error = np.mean(test_log[\"error\"])\n",
    "        print(\n",
    "            \"Test Loss: {} Test Accuracy: {}\".format(\n",
    "                round(test_loss, 4), round(1 - test_error, 4)\n",
    "            )\n",
    "        )\n",
    "        if save_predictions:\n",
    "            with open(self.logger.save_dir + \"test_predictions.p\", \"wb\") as f:\n",
    "                pickle.dump(test_log, f)\n",
    "        return test_log\n",
    "\n",
    "    def test_kaggle(self, dataset: Dataset) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"Makes predictions on the data in `datasets`, returning the loss, and\n",
    "        optionally returning the predictions and saving both.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset  test data\n",
    "        save_predictions  whether to calculate and save the predictions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a dictionary containing the loss for each data point and optionally also\n",
    "        the prediction for each data point\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _ in range(dataset.test.samples_per_epoch):\n",
    "            X, Y = dataset.test.sample()\n",
    "            Y_hat, _ = self.predict(X, Y)\n",
    "            predictions += list(np.argmax(Y_hat, axis=1))\n",
    "        kaggle = pd.DataFrame(\n",
    "            OrderedDict({\"Id\": range(len(predictions)), \"Category\": predictions})\n",
    "        )\n",
    "        kaggle.to_csv(self.logger.save_dir + \"kaggle_predictions.csv\", index=False)\n",
    "        return kaggle\n",
    "\n",
    "    def predict(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make a forward and backward pass to calculate the predictions and\n",
    "        loss of the neural network on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input features\n",
    "        Y  targets (same length as `X`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of the prediction and loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Do a forward pass. Maybe use a function you already wrote?\n",
    "        # Get the loss. Remember that the `backward` function returns the loss.\n",
    "        Y_hat = self.forward(X)\n",
    "        L = self.backward(np.array(Y), np.array(Y_hat))\n",
    "        return Y_hat, L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5 diagrams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('experiments/feed_forward_2layers_10-lr0.05_mom0.9_seed0/loss.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
